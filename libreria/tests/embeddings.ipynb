{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3a790d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers jupyter ipywidgets faiss-cpu -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cb2ee3",
   "metadata": {},
   "source": [
    "Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2dd56f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (10, 384)\n",
      "Sentence 0: [ 0.12968221  0.01845985 -0.00748047  0.06867585 -0.02418645]...\n",
      "Sentence 1: [-0.07364395  0.00870077 -0.04692024  0.03435629 -0.11529234]...\n",
      "Sentence 2: [-0.03886006 -0.03654213  0.04637926  0.05234018 -0.02747391]...\n",
      "Sentence 3: [-0.0600652   0.04444657 -0.02500929 -0.05286716 -0.06148164]...\n",
      "Sentence 4: [ 0.00511048 -0.02563424  0.0117868   0.04150654 -0.02125267]...\n",
      "Sentence 5: [ 0.00020727  0.04129593  0.0580074   0.03317517 -0.03564569]...\n",
      "Sentence 6: [ 0.00910144 -0.06488722 -0.03345207  0.01376     0.07430761]...\n",
      "Sentence 7: [ 0.06743481 -0.0599446   0.03064492  0.06001861 -0.01442595]...\n",
      "Sentence 8: [ 0.01835446  0.01416127 -0.01720778  0.04222289 -0.07529112]...\n",
      "Sentence 9: [-0.02330099  0.13033783  0.05522298  0.00189558  0.05454559]...\n"
     ]
    }
   ],
   "source": [
    "import sentence_transformers\n",
    "\n",
    "model = sentence_transformers.SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "texts = [\n",
    "    'The cat sat on the windowsill and watched the birds fly by.',\n",
    "    'Quantum computers have the potential to revolutionize cryptography.',\n",
    "    'Bananas are an excellent source of potassium and dietary fiber.',\n",
    "    'Mount Everest is the highest mountain above sea level on Earth.',\n",
    "    'The Mona Lisa is one of the most famous paintings in the world.',\n",
    "    'A solar eclipse occurs when the Moon passes between the Earth and the Sun.',\n",
    "    'Venus is the hottest planet in our solar system.',\n",
    "    'The stock market fluctuates based on investor sentiment and economic data.',\n",
    "    'Octopuses have three hearts and blue blood.',\n",
    "    'The Great Wall of China was built to protect against invasions.'\n",
    "]\n",
    "\n",
    "ids = [\n",
    "    4637834,\n",
    "    4637835,\n",
    "    4637836,\n",
    "    4637837,\n",
    "    4637838,\n",
    "    4637839,\n",
    "    4637840,\n",
    "    4637841,\n",
    "    4637842,\n",
    "    4637843\n",
    "]\n",
    "\n",
    "# Encode the sentences to get their embeddings\n",
    "embeddings = model.encode(texts)\n",
    "print(\"Embeddings shape:\", embeddings.shape)  # Should be (3, 384) for all-MiniLM-L6-v2\n",
    "for i, embedding in enumerate(embeddings):\n",
    "    print(f\"Sentence {i}: {embedding[:5]}...\")  # Print first 5 dimensions of the embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018f087c",
   "metadata": {},
   "source": [
    "Store them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "990d6bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "dim = embeddings.shape[1]  # Dimension of the embeddings\n",
    "index = faiss.IndexFlatL2(dim)  # L2 distance index\n",
    "index_with_ids = faiss.IndexIDMap(index)  # Create an index with IDs\n",
    "\n",
    "ids_np = np.array(ids, dtype=np.int64)  # Convert IDs to numpy array\n",
    "index_with_ids.add_with_ids(embeddings, ids_np)  # Add embeddings with IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e40fd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distances: [[1.6708156 1.8310041]]\n",
      "Indices: [[4637838 4637842]]\n"
     ]
    }
   ],
   "source": [
    "query = \"eiffiel\"\n",
    "query_embedding = model.encode([query])\n",
    "\n",
    "k = 2  # Number of nearest neighbors to search for\n",
    "dist, i = index_with_ids.search(query_embedding, k)  # Search for nearest neighbors\n",
    "\n",
    "print(\"Distances:\", dist)  # Distances to the nearest neighbors\n",
    "print(\"Indices:\", i)  # Indices of the nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "314b9a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append new items to the index\n",
    "new_texts = [\n",
    "    'The Eiffel Tower is located in Paris, France.',\n",
    "    'Artificial intelligence is transforming various industries.'\n",
    "]\n",
    "\n",
    "new_embeddings = model.encode(new_texts)\n",
    "index.add(new_embeddings)  # Add new embeddings to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a795bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
